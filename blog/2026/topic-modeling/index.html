<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Learning journal - my journey into topic modeling | Yongxin Lyu </title> <meta name="author" content="Yongxin Lyu"> <meta name="description" content="to the wonderland of natural language processing"> <meta name="keywords" content="Yongxin Lyu, 2D perovskites, materials science"> <meta name="google-site-verification" content="I95KO_Wu_ziNztDBkFZu69NFAiU44xx7ASnZ-Nqwrs4"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%B3&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yongxinlyu.github.io/blog/2026/topic-modeling/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yongxin</span> Lyu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Learning journal - my journey into topic modeling</h1> <p class="post-meta"> Created on January 06, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>I come from a machine learning background, mostly in the context of data science. Recently, however, I’ve become increasingly interested in <strong>natural language processing (NLP)</strong>.</p> <p>During my PhD, I heard about NLP and large language models (LLMs) all the time — from the everyday use of ChatGPT to research applications of language models in materials science. When I started job hunting, this interest was reinforced even more: many roles explicitly require experience with NLP or LLMs. Looking back now, I fully understand why.</p> <p>At the time, though, I was sceptical. I felt that NLP and LLMs were surrounded by hype, and I couldn’t see an immediate or practical use for them in my own research workflow. I tended to associate NLP almost exclusively with <strong>text generation</strong>, which wasn’t something I thought I needed.</p> <p>That perspective changed recently.</p> <p>In this post, I want to document my learning journey into <strong>topic modelling</strong> as a beginner: what motivated me to explore NLP more seriously, where I initially felt lost, and how I started building a clearer mental model of this powerful technique. My hope is that this might be useful—or at least encouraging—for others who are curious about NLP but unsure where to begin.</p> <hr> <h3 id="why-i-became-interested-in-nlp"><strong>Why I Became Interested in NLP</strong></h3> <p>Very recently, I realised something that now feels obvious in hindsight: <strong>language and text are also forms of data</strong>—and increasingly, they are the <em>main</em> form of data I interact with.</p> <p>Job descriptions, research proposals, funding schemes, papers across different disciplines, and descriptions of research areas I might want to move into: almost everything I needed to process was text. Reading, filtering, and synthesising all of this information took a huge amount of time.</p> <p>I started asking myself:</p> <blockquote> <p>What if there were tools that could help me process and understand large collections of text more efficiently?</p> </blockquote> <p>This was when I began to seriously think about <strong>treating text as data</strong>. Importantly, I also realised that NLP isn’t only about <em>generating</em> text. An equally important—and, for me, more relevant—part of NLP is about <strong>understanding, organising, and extracting structure from text</strong>.</p> <p>That shift in perspective was what ultimately drew me in.</p> <hr> <h3 id="learning-topic-modelling"><strong>Learning Topic Modelling</strong></h3> <p>Once I decided to explore NLP more seriously, topic modelling quickly caught my attention. It promised a way to automatically discover structure in large collections of documents. For example, imagine being able to quickly group research papers into themes and get a high-level sense of emerging or “hot” topics in a field. That alone could save a huge amount of time.</p> <p>There is no shortage of information online. ChatGPT can generate plenty of code snippets and explanations, and I started exploring this space during the Christmas holiday, on the trains to our hiking destinations.</p> <p>However, like acquiring any new skills in this era of chatbots, I quickly ran into a problem: the information felt <strong>fragmented</strong>.</p> <p>The code ChatGPT produced often ran correctly, but I didn’t always understand:</p> <ul> <li> <em>why</em> it worked, which part is essential and which can be replaced,</li> <li> <em>how</em> to interpret the output, or</li> <li> <em>what meaningful insight I should actually extract from the results</em>.</li> </ul> <p>What I was really looking for was a <strong>blueprint</strong>—not just instructions for running code, but a systematic understanding of the underlying ideas. I wanted something that could bridge what I already knew about machine learning with this new area of NLP.</p> <p>Eventually, I found a few books that helped me build this conceptual foundation, alongside ChatGPT, which was still incredibly useful for experimentation and clarification:</p> <ul> <li> <p><strong>The Handbook of NLP with Gensim</strong> by Chris Kuo</p> <p>This was by far the most helpful resource for me. The author explains both the core concepts and the implementation details, with just enough mathematics to understand what is happening under the hood. I also really enjoyed the tone—it’s technical but approachable, and surprisingly fun to read.</p> </li> <li> <p><strong>Hands-On Large Language Models by Jay Alammar &amp; Maarten grootendorst</strong></p> <p>I generally like O’Reilly books for learning machine learning topics, and this one was no exception. Despite the title, the book starts from language models more broadly, both small and large. I didn’t read it cover to cover, but the introductory chapters gave me a strong high-level blueprint of NLP and how different language modelling approaches fit together.</p> </li> <li> <p><strong>Practical Natural Language Processing</strong></p> <p>Another O’Reilly book, with a clear and structured explanation of topic modelling. I found it particularly helpful for understanding how different authors and practitioners approach NLP as a whole, and where topic modelling sits within that broader landscape.</p> </li> </ul> <hr> <h3 id="my-beginners-understanding-of-topic-modelling"><strong>My Beginner’s Understanding of Topic Modelling</strong></h3> <p>As a beginner, this is how I currently understand topic modelling.</p> <p>There are many approaches:</p> <ul> <li> <strong>Classical methods</strong>, such as <em>Latent Semantic Analysis (LSA)</em> and <em>Latent Dirichlet Allocation (LDA)</em> </li> <li> <strong>More modern methods</strong>, such as <em>BERTopic</em>, which rely on transformer-based embeddings</li> </ul> <p>To me, this distinction feels similar to <strong>machine learning vs. deep learning</strong>.</p> <p>Classical topic models:</p> <ul> <li>are more interpretable,</li> <li>require fewer computational resources,</li> <li>but may sacrifice some performance or flexibility.</li> </ul> <p>Modern, embedding-based methods:</p> <ul> <li>often produce cleaner-looking topics,</li> <li>handle short or noisy text better,</li> <li>but are less interpretable and more computationally expensive.</li> </ul> <p>For my own learning goals, I’m currently leaning towards the <strong>classical approaches</strong>. Even though methods like LDA have been around for many years, they provide a strong conceptual foundation. I see them as an important baseline to understand before moving on to more complex models.</p> <p>This blog, therefore, mainly documents my learning journey (so far) with <strong>classical topic modelling</strong>, especially LDA.</p> <hr> <h3 id="my-first-project-discovery-projects"><strong>My First Project: Discovery Projects</strong></h3> <p>My first hands-on project applies topic modelling to <strong>Discovery Projects from the 2023–2026 funding rounds</strong>. Like many researchers who have spent a large part of their academic life chasing funding, I believe that being aware of emerging trends—and understanding what kinds of topics attract funding—is extremely important. This made the dataset immediately interesting to me.</p> <p>The goal is to cluster around <strong>2,000 project summaries across four years</strong> into different topics based on their text descriptions.</p> <p>This turned out to be more challenging than I initially expected:</p> <ul> <li>the summaries are relatively short,</li> <li>the language is highly domain-specific,</li> <li>and many concepts are expressed implicitly rather than explicitly.</li> </ul> <p>All of these factors make it harder for topic models to identify clean, well-separated themes. Still, I decided to stick with this dataset and see how far I could push the approach.</p> <hr> <h3 id="preprocessing-where-most-of-the-work-happens"><strong>Preprocessing: Where Most of the Work Happens</strong></h3> <p>One key lesson I learned very quickly is that <strong>topic modelling is mostly about preprocessing</strong>.</p> <p>There are many packages and strategies available, and the choices you make here can dramatically affect the results. My current preprocessing pipeline looks like this:</p> <ol> <li> <strong>Text normalization</strong> <ul> <li>lowercase</li> <li>remove punctuation</li> <li>remove numbers</li> <li>normalize whitespace</li> </ul> </li> <li> <strong>Lemmatization</strong> <ul> <li>I avoid stemming, as it hurts interpretability</li> </ul> </li> <li> <strong>Stopword removal</strong> <ul> <li>generic stopwords</li> <li>domain-specific stopwords</li> </ul> </li> <li> <strong>Bigram construction</strong> <ul> <li>to capture multi-word concepts</li> </ul> </li> <li> <strong>Dictionary-level filtering</strong> <ul> <li>remove extremely common words</li> <li>remove very rare words</li> </ul> </li> </ol> <p>Once this preprocessing is done, the bag-of-words representation and LDA modelling itself are actually <strong>surprisingly simple</strong>.</p> <p>[jupyternotebook to be inserted]</p> <p>The resulting topics make sense to me. For example, topics related to biology appear extremely prominent, and artificial intelligence also clearly emerges as a major theme.</p> <hr> <h3 id="where-i-am-now"><strong>Where I Am Now</strong></h3> <p>Using this workflow, I can obtain topic modelling results that are <strong>reasonable</strong>, but not yet fully satisfying. Some topics still feel vague or difficult to interpret, and I’m continuing to refine the preprocessing and modelling choices.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/job-hunting/">Finding my employability after the PhD</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/perovskite-diy-model/">DIY Perovskite crystal model - Origami, knitting and 3D printing</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/my-AI4Science-journey/">From a distant dream to reality - my AI4Science journey</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/blog-update/">A new chapter to my blog</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/life-after-phd/">Life after PhD</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Yongxin Lyu. Last updated: January 06, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>