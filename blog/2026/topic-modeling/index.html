<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Learning journal - my journey into topic modeling | Yongxin Lyu </title> <meta name="author" content="Yongxin Lyu"> <meta name="description" content="to the wonderland of natural language processing"> <meta name="keywords" content="Yongxin Lyu, 2D perovskites, materials science"> <meta name="google-site-verification" content="I95KO_Wu_ziNztDBkFZu69NFAiU44xx7ASnZ-Nqwrs4"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%B3&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yongxinlyu.github.io/blog/2026/topic-modeling/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yongxin</span> Lyu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Learning journal - my journey into topic modeling</h1> <p class="post-meta"> Created on January 06, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>I come from a machine learning background, mostly in the context of data science. Recently, however, I’ve become increasingly interested in <strong>natural language processing (NLP)</strong>.</p> <p>During my PhD, I constantly heard about NLP and large language models (LLMs). Everyone was using ChatGPT and amazed by how good it is. At the same time, there was an explosion of research papers on arXiv across computer science, materials science, and beyond, with some papers suggesting that models like GPT could help discover new materials with better properties. When I started job hunting, many roles explicitly required experience with NLP or LLMs. Looking back now, I fully understand why.</p> <p>At the time, though, I was sceptical. I felt that NLP and LLMs were surrounded by hype, and I couldn’t see an immediate or practical use for them in my own research workflow. I tended to associate NLP almost exclusively with <strong>text generation</strong>, which wasn’t something I thought I needed.</p> <p>That perspective has changed.</p> <p>In this post, I want to document my learning journey into <strong>NLP</strong> as a beginner: what motivated me to explore NLP more seriously, how I built a clearer mental model of this field over the past three months, and the results of my first NLP project—topic modelling of ARC Discovery Projects.</p> <hr> <h3 id="why-i-became-interested-in-nlp"><strong>Why I Became Interested in NLP</strong></h3> <p>Very recently, I realised something that now feels obvious in hindsight:</p> <blockquote> <p>Language and text are also forms of data, and increasingly, they are the main form of data I interact with.</p> </blockquote> <p>Job descriptions, research proposals, funding schemes, academic papers across disciplines, and descriptions of research areas I might want to move into—almost everything I needed to process was text. Reading, filtering, and synthesising all of this information took a huge amount of time.</p> <p>I started asking myself:</p> <blockquote> <p>There must be tools that could help me process and understand large collections of text more efficiently.</p> </blockquote> <p>This was when I began to seriously think about <strong>treating text as data</strong>. At first, I didn’t even know this fell under NLP. I simply searched for Python packages that could process text in a structured, data-driven way. To my surprise (and delight), I discovered that this is exactly what NLP is about.</p> <p>Over time, I also realised that <strong>LLMs are only one branch of NLP</strong>, albeit the most visible one. Training or fine-tuning something like ChatGPT may feel out of reach, but NLP as a field is much broader and far more accessible. A word cloud is a very simple NLP technique. LinkedIn job recommendations rely heavily on NLP. There are many flavours of NLP beyond text generation.</p> <p>For me, the most relevant part of NLP is about <strong>understanding, organising, and extracting structure from text</strong>.</p> <hr> <h3 id="learning-topic-modelling"><strong>Learning Topic Modelling</strong></h3> <p>Over the past few months, I’ve been actively job hunting. Postdoctoral roles, in particular, feel like they come with an almost unlimited number of possibilities—which is exciting, but also intimidating. As I applied for more positions, I started wondering: <em>What exactly is the job space I’m exploring? Are there hidden structures or dominant themes?</em></p> <p>If you’re familiar with unsupervised learning, this idea is similar to <strong>dimensionality reduction</strong>—compressing high-dimensional data into two dimensions that can be visualised. Conceptually, topic modelling felt very familiar to me from a machine learning perspective. The main difference lies in <strong>how text is transformed into numerical features</strong>.</p> <p>There is no shortage of information online. ChatGPT can generate plenty of code snippets and explanations, and I used it extensively. I asked ChatGPT to generate code for transforming job descriptions into numerical representations that could be fed into machine learning models. The results were quite good, and I shared some of this in my <a href="insert%20link">previous blog post</a>.</p> <p>However, like acquiring any new skill in the era of chatbots, the information I encountered was <strong>fragmented</strong>. I didn’t always understand <em>why</em> things worked, what the results really meant, or how different methods fit together within the broader NLP landscape.</p> <p>I decided to slow down and explore this field more seriously during the Christmas holiday. Gradually, I learned that what I was doing fell under <strong>topic modelling</strong>, a subfield of NLP that aims to automatically discover structure in large collections of documents.</p> <p>The promise was compelling. Imagine being able to quickly group thousands of research papers into themes and get a high-level sense of emerging or “hot” topics in a field. That alone could save an enormous amount of time.</p> <p>To build a more systematic understanding—something that could bridge what I already knew about machine learning with this new NLP domain—I turned to a few books, alongside continued experimentation with ChatGPT:</p> <ul> <li> <p><strong>The Handbook of NLP with Gensim</strong> by Chris Kuo</p> <p>By far the most helpful resource for me. It explains both the core concepts and implementation details, with just enough mathematics to understand what’s happening under the hood. The tone is technical but approachable, and surprisingly fun to read.</p> </li> <li> <p><strong>Hands-On Large Language Models</strong> by Jay Alammar &amp; Maarten grootendorst</p> <p>I generally like O’Reilly books for hands-on machine learning topics, and this one was no exception. Despite the title, the book starts from language models more broadly, both small and large. I didn’t read it cover to cover, but the introductory chapters gave me a strong high-level blueprint of NLP and how different language modelling approaches fit together.</p> </li> <li> <p><strong>Practical Natural Language Processing</strong> by Sowmya Vajjala et al.</p> <p>Another O’Reilly book, with a clear and structured explanation of NLP. I found it particularly helpful for understanding how different authors and practitioners approach NLP as a whole, and where topic modelling sits within that broader landscape.</p> </li> </ul> <hr> <h3 id="my-beginners-understanding-of-topic-modelling"><strong>My Beginner’s Understanding of Topic Modelling</strong></h3> <p>After reading books, papers, blog posts, and experimenting extensively (with a lot of help from ChatGPT), I now have a <strong>preliminary but coherent mental model</strong> of topic modelling.</p> <p>Broadly, there are two families of approaches:</p> <ul> <li> <strong>Classical methods</strong>, such as <em>Latent Semantic Analysis (LSA)</em> and <em>Latent Dirichlet Allocation (LDA)</em> </li> <li> <strong>Modern methods</strong>, such as <em>BERTopic</em>, which rely on transformer-based embeddings</li> </ul> <p>To me, this distinction feels very similar to <strong>machine learning vs. deep learning</strong>.</p> <p>Classical topic models are more interpretable, require fewer computational resources, but may sacrifice some flexibility or performance.</p> <p>Embedding-based methods often produce cleaner-looking topics, handle short or noisy text better, but are less interpretable and more computationally expensive.</p> <p>For my learning goals, I’m currently leaning towards <strong>classical approaches</strong>, especially LDA. Even though these models have been around for many years, they provide a strong conceptual foundation. I see them as essential baselines to understand before moving on to more complex models.</p> <p>The section below therefore mainly documents my learning journey (so far) with <strong>classical topic modelling</strong>.</p> <hr> <h3 id="my-first-project-discovery-projects"><strong>My First Project: Discovery Projects</strong></h3> <p>The fastest way for me to learn is through a concrete project. I chose <strong>ARC Discovery Projects from the 2023–2026 funding rounds</strong> as my first formal NLP task.</p> <p>Like many researchers who have spent a large part of their academic life chasing funding, I believe that understanding <strong>emerging trends</strong>—and what kinds of topics attract funding—is extremely important.</p> <p>The dataset consists of <strong>1,961 project summaries across four years</strong>, and the goal is to cluster them into topics based on their text descriptions.</p> <p>This turned out to be more challenging than I initially expected: the summaries are relatively short, the language is highly domain-specific, and many concepts are expressed implicitly rather than explicitly.</p> <p>All of these factors make it harder for topic models to identify clean, well-separated themes. Still, I decided to stick with this dataset and see how far I could push the approach.</p> <hr> <h3 id="preprocessing-where-most-of-the-work-happens"><strong>Preprocessing: Where Most of the Work Happens</strong></h3> <p>One lesson became clear very quickly: <strong>topic modelling is mostly about preprocessing—and interpretation</strong>. The model itself is often the easy part; the hard work lies in deciding <em>what</em> information to keep, <em>what</em> to discard, and <em>how</em> to represent text in a way that preserves meaning.</p> <p>My current preprocessing pipeline looks like this:</p> <ol> <li> <p><strong>Text normalisation</strong>: Lowercasing, removing punctuation, and stripping out numbers to reduce superficial variation.</p> </li> <li> <p><strong>Lemmatization</strong>: For example, <em>projects → project</em>. I deliberately avoid stemming, as it often damages interpretability by producing unnatural word forms.</p> </li> <li> <p><strong>Stopword removal</strong>: I remove generic stopwords but intentionally keep domain-specific ones for now. Since I later rely on a Termite-like topic–term matrix, preserving these terms helps with interpretation.</p> </li> <li> <p><strong>Bigram construction</strong>: To capture multi-word concepts such as <em>“artificial intelligence”</em>, which would otherwise be split and lose semantic coherence.</p> </li> </ol> <p>Once these steps are complete, the actual Bag-of-Words construction and LDA modelling are <strong>surprisingly simple</strong>.</p> <p>There are many sophisticated NLP libraries available—such as gensim and nltk—and I did experiment with them. However, I chose to stick largely to <strong>my own preprocessing scripts combined with scikit-learn</strong>, prioritising transparency and interpretability over abstraction. At this stage of learning, understanding <em>exactly</em> what happens at each step felt more valuable than convenience.</p> <hr> <h3 id="interpreting-the-tf-idf--pca-visualisation"><strong>Interpreting the TF-IDF + PCA Visualisation</strong></h3> <p>Using the preprocessing pipeline above, I extracted <strong>TF-IDF features</strong> from all project summaries and projected them into two dimensions using <strong>PCA</strong>, a linear dimensionality reduction technique.</p> <iframe src="/assets/img/blog_figure/topic-modeling-pca-FOR.html" width="100%" height="500" frameborder="0"></iframe> <p>In the resulting visualisation, each point represents a project. I colour-coded the points by their <strong>Field of Research (FOR)</strong> classification. When filtering or hovering over different FOR codes, several interesting patterns emerge.</p> <p><strong>How I interpret this result</strong></p> <ul> <li>Projects from <strong>closely related research fields tend to cluster together</strong>, suggesting that TF-IDF captures meaningful semantic similarity. For instance, <em>Biomedical and Clinical Sciences</em> frequently overlap with <em>Biological Sciences</em>, which aligns well with intuition.</li> <li>Importantly, this structure emerges <strong>without any supervision</strong>. The model is not told about disciplines or categories, yet it recovers them purely from text. This indicates that the project descriptions contain a strong and consistent signal.</li> <li>The overall project space forms a <strong>rough triangular shape</strong>: <ul> <li>one corner dominated by biology-related fields,</li> <li>another by chemical sciences and engineering,</li> <li>and a third by social sciences.</li> </ul> </li> </ul> <p>This triangular structure likely reflects <strong>fundamentally different vocabularies and conceptual frameworks</strong> across these broad domains. Even after aggressive dimensionality reduction, these linguistic differences remain visible, highlighting just how distinct disciplinary language can be.</p> <p>Overall, this visualisation was an important sanity check. It reassured me that:</p> <ul> <li>the preprocessing choices are reasonable,</li> <li>TF-IDF is capturing meaningful similarities between projects,</li> <li>and topic modelling on this dataset is worth pursuing further.</li> </ul> <hr> <h3 id="discovering-underlying-topics-lda-results"><strong>Discovering underlying topics: LDA Results</strong></h3> <p>After exploring the global structure with TF-IDF and PCA, I moved on to <strong>Bag-of-Words + LDA</strong> to extract more explicit, interpretable topics.</p> <p>I experimented with different numbers of topics and eventually settled on <strong>six</strong>, balancing granularity and interpretability. To visualise the results, I used a <strong>Termite-like topic–term matrix</strong>, a design originally proposed by researchers at Stanford.</p> <p>The resulting matrix is genuinely fascinating. Some themes are immediately recognisable. For example, <strong>artificial intelligence</strong> clearly emerges as a dominant topic, reflecting its growing prominence across many funding areas rather than being confined to a single discipline.</p> <p>That said, interpreting LDA outputs is still something I’m actively learning. Unlike PCA plots, which offer a relatively intuitive geometric interpretation, topic–term matrices require <strong>subjective judgement and domain knowledge</strong>. Deciding where one topic ends and another begins is not always clear-cut.</p> <p>At this stage, I see this ambiguity not as a weakness, but as part of the learning process. Topic modelling doesn’t hand you answers—it invites you to <em>reason</em> about patterns in text.</p> <hr> <h3 id="closing-thoughts"><strong>Closing Thoughts</strong></h3> <p>This project has fundamentally changed how I think about NLP. I no longer see it as “just text generation,” but as a powerful toolkit for <strong>making sense of unstructured text at scale</strong>.</p> <p>Topic modelling, in particular, feels like a natural extension of the unsupervised learning techniques I was already familiar with—just applied to a different and increasingly important kind of data. While I’m still very much a beginner, this journey has already reshaped how I approach text, research trends, and even my own career exploration.</p> <p>In future posts, I plan to dive deeper into topic interpretation, compare classical and embedding-based approaches, and reflect on where NLP fits into my broader research toolkit.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/job-hunting/">Finding my employability after the PhD</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/perovskite-diy-model/">DIY Perovskite crystal model - Origami, knitting and 3D printing</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/my-AI4Science-journey/">From a distant dream to reality - my AI4Science journey</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/blog-update/">A new chapter to my blog</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/life-after-phd/">Life after PhD</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Yongxin Lyu. Last updated: January 07, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>